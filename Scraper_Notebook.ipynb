{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through inspecting the page it looks like I can get all the data I need here.  I am going to get the sub page information as well but for now just making sure I can get everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.pro-football-reference.com/years/2019/receiving.htm#'\n",
    "\n",
    "page = requests.get(url)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the page it looks like there is only one `tbody` and that is where the data I want is.  So after checking that this is true we can move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(soup.find_all('tbody'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a given season page seems to only have one table so we are good.  Now to check what a row looks like and make sure the data I want is in there.  I also need to find the url link for the player pages as I want those as well.\n",
    "\n",
    "Finally it looks like there are rows with missing data (for viewing ease) I will need to make sure to skip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find_all('tbody')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = table.find_all('tr')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table.find_all('tr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<td class=\"left\" csk=\"McCaffrey,Christian\" data-append-csv=\"McCaCh01\" data-stat=\"player\"><a href=\"/players/M/McCaCh01.htm\">Christian McCaffrey </a>*+</td>,\n",
       " <td class=\"left\" data-stat=\"team\"><a href=\"/teams/car/2019.htm\" title=\"Carolina Panthers\">CAR</a></td>,\n",
       " <td class=\"right\" data-stat=\"age\">23</td>,\n",
       " <td class=\"left\" data-stat=\"pos\">RB</td>,\n",
       " <td class=\"right\" data-stat=\"g\">16</td>,\n",
       " <td class=\"right\" data-stat=\"gs\">16</td>,\n",
       " <td class=\"right\" data-stat=\"targets\">142</td>,\n",
       " <td class=\"right\" data-stat=\"rec\">116</td>,\n",
       " <td class=\"right\" data-stat=\"catch_pct\">81.7%</td>,\n",
       " <td class=\"right\" data-stat=\"rec_yds\">1005</td>,\n",
       " <td class=\"right\" data-stat=\"rec_yds_per_rec\">8.7</td>,\n",
       " <td class=\"right\" data-stat=\"rec_td\">4</td>,\n",
       " <td class=\"right\" data-stat=\"rec_first_down\">58</td>,\n",
       " <td class=\"right\" data-stat=\"rec_long\">28</td>,\n",
       " <td class=\"right\" data-stat=\"rec_yds_per_tgt\">7.1</td>,\n",
       " <td class=\"right\" data-stat=\"rec_per_g\">7.3</td>,\n",
       " <td class=\"right\" data-stat=\"rec_yds_per_g\">62.8</td>,\n",
       " <td class=\"right\" data-stat=\"fumbles\">1</td>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.find_all('td')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have the right amount of row data and each row looks to have the data I want.  So I should be good to move to the next step.  One thing to note is that we have sub pages to scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_one = row.find('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = col_one.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/players/M/McCaCh01.htm'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player.get_attribute_list('href')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'player'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_one.get('data-stat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "position = 'receiving'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.pro-football-reference.com/years/2005/receiving.htm#\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient()\n",
    "db = client['nfl']\n",
    "season_collection = db['season_raw']\n",
    "player_collection = db['player_raw']\n",
    "\n",
    "for year in range(2005, 2006):\n",
    "    url = 'https://www.pro-football-reference.com/years/{}/{}.htm#'.format(year, position)\n",
    "    print(url)\n",
    "    \n",
    "    page = requests.get(url)\n",
    "    \n",
    "    # save the raw HTML so I never have to scrape again (just reparese)\n",
    "    season_collection.insert_one({'year':year,'position':position, 'html':page.text})\n",
    "    \n",
    "    soup = BeautifulSoup(page.text)\n",
    "    \n",
    "    # Get all player sub pages:\n",
    "    table = soup.find('tbody')\n",
    "    for row in table.find_all('tr'):\n",
    "        col_one = row.find('td')\n",
    "        if col_one == None:\n",
    "            continue\n",
    "        player = col_one.find('a')\n",
    "        url_player = player.get_attribute_list('href')[0]\n",
    "        \n",
    "        sub_url = 'https://www.pro-football-reference.com{}'.format(url_player)\n",
    "        sub_page = requests.get(sub_url)\n",
    "        \n",
    "        wtf.append([sub_page.status_code, url_player])\n",
    "        \n",
    "        player_collection.insert_one({'url':sub_url, \n",
    "                                      'player': url_player.split('/')[-1],\n",
    "                                      'player_url': url_player,\n",
    "                                      'html':sub_page.text,\n",
    "                                      'position':position,\n",
    "                                      'year':year})\n",
    "        sleep(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
